{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas numpy osmnx requests scikit-learn matplotlib seaborn folium streamlit-folium reportlab imblearn joblib\n",
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlEi-LSLkHO-",
        "outputId": "02aaf858-0162-4d6d-927b-e89672ddab3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.50.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: osmnx in /usr/local/lib/python3.12/dist-packages (2.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: folium in /usr/local/lib/python3.12/dist-packages (0.20.0)\n",
            "Requirement already satisfied: streamlit-folium in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.4)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.12/dist-packages (0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: geopandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from osmnx) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.12/dist-packages (from osmnx) (3.5)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.12/dist-packages (from osmnx) (2.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from folium) (0.8.1)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.12/dist-packages (from folium) (3.1.6)\n",
            "Requirement already satisfied: xyzservices in /usr/local/lib/python3.12/dist-packages (from folium) (2025.4.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (from imblearn) (0.14.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.5.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas>=1.0.1->osmnx) (0.11.1)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas>=1.0.1->osmnx) (3.7.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.9->folium) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPfnshDo_M66",
        "outputId": "2e0a86d4-821e-476f-d2d3-df3a2896ea8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import osmnx as ox\n",
        "import requests\n",
        "from datetime import datetime, timezone, timedelta\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             confusion_matrix, classification_report)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import folium\n",
        "from streamlit_folium import st_folium\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "import io\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# --- Constants ---\n",
        "POLLUTANTS = [\"pm25\", \"pm10\", \"no2\", \"co\", \"so2\", \"o3\"]\n",
        "OPENWEATHER_KEY = \"10a5441a14aa1e7fca5f727340992700\"\n",
        "THRESHOLDS = {\"pm25\": 50, \"pm10\": 100, \"no2\": 80, \"co\": 10000, \"so2\": 75, \"o3\": 70}\n",
        "EMAIL_CONFIG = {\n",
        "    \"sender\": \"tabrez0687@gmail.com\",\n",
        "    \"password\": \"plgl knjh cyit oprz\",\n",
        "    \"receiver\": \"vu.241fa04c38@gmail.com\",\n",
        "    \"server\": \"smtp.gmail.com\",\n",
        "    \"port\": 587\n",
        "}\n",
        "\n",
        "# --- Custom CSS ---\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .main {\n",
        "        background-color: #f0f4f8;\n",
        "        padding: 20px;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .stApp > header {\n",
        "        background-color: #2e7d32;\n",
        "        color: white;\n",
        "        padding: 10px;\n",
        "        text-align: center;\n",
        "        border-bottom: 2px solid #1b5e20;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: 'Arial', sans-serif;\n",
        "        font-size: 2.5em;\n",
        "        color: #ffffff;\n",
        "        text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\n",
        "    }\n",
        "    .sidebar .sidebar-content {\n",
        "        background-color: #e8f5e9;\n",
        "        padding: 20px;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .sidebar .stButton>button {\n",
        "        background-color: #2e7d32;\n",
        "        color: white;\n",
        "        border: none;\n",
        "        padding: 10px 20px;\n",
        "        border-radius: 5px;\n",
        "        transition: background-color 0.3s;\n",
        "    }\n",
        "    .sidebar .stButton>button:hover {\n",
        "        background-color: #1b5e20;\n",
        "    }\n",
        "    .card {\n",
        "        background-color: white;\n",
        "        padding: 15px;\n",
        "        margin-bottom: 20px;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .stTextInput>input, .stNumberInput>input, .stSelectbox>select, .stDateInput>input {\n",
        "        border: 1px solid #ccc;\n",
        "        border-radius: 5px;\n",
        "        padding: 5px;\n",
        "    }\n",
        "    .stRadio label {\n",
        "        font-size: 1em;\n",
        "        color: #333;\n",
        "    }\n",
        "    .stAlert {\n",
        "        border-radius: 5px;\n",
        "        padding: 10px;\n",
        "    }\n",
        "    .stPlotlyChart, .stGraph {\n",
        "        border: 1px solid #ddd;\n",
        "        border-radius: 5px;\n",
        "        padding: 10px;\n",
        "    }\n",
        "    .stDownloadButton>button {\n",
        "        background-color: #2e7d32;\n",
        "        color: white;\n",
        "        border: none;\n",
        "        padding: 10px 20px;\n",
        "        border-radius: 5px;\n",
        "        transition: background-color 0.3s;\n",
        "    }\n",
        "    .stDownloadButton>button:hover {\n",
        "        background-color: #1b5e20;\n",
        "    }\n",
        "    .insight-box {\n",
        "        background-color: #e8f5e9;\n",
        "        padding: 15px;\n",
        "        border-radius: 10px;\n",
        "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
        "        margin-top: 10px;\n",
        "    }\n",
        "    .insight-box h4 {\n",
        "        color: #2e7d32;\n",
        "        margin-bottom: 5px;\n",
        "    }\n",
        "    .insight-box p {\n",
        "        margin-bottom: 10px;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def get_weather(lat, lon, api_key):\n",
        "    url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
        "    params = {\"lat\": lat, \"lon\": lon, \"appid\": api_key, \"units\": \"metric\"}\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json() if response.status_code == 200 else {}\n",
        "\n",
        "def extract_osm_features(lat, lon, radius=2000):\n",
        "    features = {\"roads_count\": 0, \"industries_count\": 0, \"farms_count\": 0, \"dumps_count\": 0}\n",
        "    point = (lat, lon)\n",
        "    try:\n",
        "        roads = ox.features_from_point(point, tags={\"highway\": True}, dist=radius)\n",
        "        features[\"roads_count\"] = len(roads)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        industries = ox.features_from_point(point, tags={\"landuse\": [\"industrial\", \"commercial\"]}, dist=radius)\n",
        "        features[\"industries_count\"] = len(industries)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        farms = ox.features_from_point(point, tags={\"landuse\": [\"farmland\", \"farm\", \"agricultural\"]}, dist=radius)\n",
        "        features[\"farms_count\"] = len(farms)\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        dumps = ox.features_from_point(point, tags={\"landuse\": [\"landfill\", \"waste\", \"dump\"]}, dist=radius)\n",
        "        features[\"dumps_count\"] = len(dumps)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return features\n",
        "\n",
        "def build_dataset(city, lat, lon, aq_csv_file, openweather_key):\n",
        "    try:\n",
        "        df_aq = pd.read_csv(aq_csv_file, skiprows=2, on_bad_lines=\"skip\", engine=\"python\")\n",
        "        df_aq = df_aq.loc[:, ~df_aq.columns.str.contains(\"^Unnamed\")]\n",
        "        df_aq[\"source\"] = \"OpenAQ\"\n",
        "    except Exception as e:\n",
        "        st.error(f\"⚠️ Failed to load AQ CSV: {e}\")\n",
        "        return pd.DataFrame(), {}\n",
        "\n",
        "    if \"latitude\" not in df_aq.columns or \"longitude\" not in df_aq.columns:\n",
        "        df_aq[\"latitude\"] = lat\n",
        "        df_aq[\"longitude\"] = lon\n",
        "\n",
        "    df_aq.loc[df_aq[\"parameter\"] == \"co\", \"value\"] *= 1144.6\n",
        "    df_agg = (\n",
        "        df_aq.groupby([\"location_name\", \"latitude\", \"longitude\", \"datetimeUtc\", \"parameter\"])[\"value\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "    )\n",
        "    df_wide = df_agg.pivot_table(\n",
        "        index=[\"location_name\", \"latitude\", \"longitude\", \"datetimeUtc\"],\n",
        "        columns=\"parameter\",\n",
        "        values=\"value\",\n",
        "        aggfunc=\"mean\",\n",
        "        fill_value=np.nan,\n",
        "    ).reset_index()\n",
        "\n",
        "    for pollutant in POLLUTANTS:\n",
        "        if pollutant not in df_wide.columns:\n",
        "            df_wide[pollutant] = np.nan\n",
        "\n",
        "    df_wide = df_wide.sort_values([\"location_name\", \"datetimeUtc\"])\n",
        "    df_wide[POLLUTANTS] = (\n",
        "        df_wide.groupby(\"location_name\")[POLLUTANTS].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    )\n",
        "\n",
        "    unique_locations = df_wide[[\"location_name\", \"latitude\", \"longitude\"]].drop_duplicates()\n",
        "    osm_dict = {\n",
        "        (row[\"location_name\"], row[\"latitude\"], row[\"longitude\"]): extract_osm_features(\n",
        "            row[\"latitude\"], row[\"longitude\"]\n",
        "        )\n",
        "        for _, row in unique_locations.iterrows()\n",
        "    }\n",
        "    df_osm = df_wide.apply(\n",
        "        lambda row: pd.Series(\n",
        "            osm_dict.get(\n",
        "                (row[\"location_name\"], row[\"latitude\"], row[\"longitude\"]),\n",
        "                {\"roads_count\": 0, \"industries_count\": 0, \"farms_count\": 0, \"dumps_count\": 0},\n",
        "            )\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "    df_wide = pd.concat([df_wide, df_osm], axis=1)\n",
        "\n",
        "    weather_data = get_weather(lat, lon, openweather_key)\n",
        "    weather_features = {\n",
        "        \"temp_c\": weather_data.get(\"main\", {}).get(\"temp\"),\n",
        "        \"humidity\": weather_data.get(\"main\", {}).get(\"humidity\"),\n",
        "        \"pressure\": weather_data.get(\"main\", {}).get(\"pressure\"),\n",
        "        \"wind_speed\": weather_data.get(\"wind\", {}).get(\"speed\"),\n",
        "        \"wind_dir\": weather_data.get(\"wind\", {}).get(\"deg\"),\n",
        "        \"weather_source\": \"OpenWeatherMap\",\n",
        "    }\n",
        "    for k, v in weather_features.items():\n",
        "        df_wide[k] = v\n",
        "\n",
        "    df_wide[\"aqi_proxy\"] = (\n",
        "        df_wide.get(\"pm25\", 0) * 0.4\n",
        "        + df_wide.get(\"pm10\", 0) * 0.3\n",
        "        + df_wide.get(\"no2\", 0) * 0.2\n",
        "        + df_wide.get(\"co\", 0) * 0.1\n",
        "    )\n",
        "    df_wide[\"pollution_per_road\"] = df_wide.get(\"pm25\", 0) / (df_wide.get(\"roads_count\", 1) + 1)\n",
        "\n",
        "    meta = {\n",
        "        \"city\": city,\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"records\": len(df_wide),\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"source\": \"OpenAQ\",\n",
        "        \"unique_stations\": df_wide[\"location_name\"].nunique(),\n",
        "    }\n",
        "    return df_wide, meta\n",
        "\n",
        "def save_datasets(df, filename):\n",
        "    if df is None or (isinstance(df, pd.DataFrame) and df.empty):\n",
        "        st.warning(f\"{filename} is empty. Skipping save.\")\n",
        "        return\n",
        "    if isinstance(df, dict):\n",
        "        df = pd.DataFrame([df])\n",
        "    df.to_csv(f\"{filename}.csv\", index=False)\n",
        "    df.to_json(f\"{filename}.json\", orient=\"records\", indent=2)\n",
        "    st.success(f\"Saved {filename}.csv and {filename}.json\")\n",
        "\n",
        "def consolidate_dataset(df_aq, df_meta, filename):\n",
        "    if df_aq is None or df_aq.empty:\n",
        "        st.warning(\"AQ dataset empty, skipping consolidation.\")\n",
        "        return\n",
        "    for k, v in df_meta.items():\n",
        "        df_aq[k] = v\n",
        "    df_aq.to_csv(f\"{filename}.csv\", index=False)\n",
        "    st.success(f\"Consolidated dataset saved as {filename}.csv\")\n",
        "\n",
        "def label_source(row):\n",
        "    pm25 = row.get(\"pm25\", 0)\n",
        "    roads = row.get(\"roads_count\", 0)\n",
        "    industries = row.get(\"industries_count\", 0)\n",
        "    farms = row.get(\"farms_count\", 0)\n",
        "    if pd.notna(pm25) and pm25 > 25 and industries > 0:\n",
        "        return \"Industrial\"\n",
        "    elif pd.notna(pm25) and pm25 > 15 and roads > 5:\n",
        "        return \"Traffic\"\n",
        "    elif farms > 0:\n",
        "        return \"Agricultural\"\n",
        "    return \"Mixed/Other\"\n",
        "\n",
        "def generate_pdf_report(df, filename, time_range):\n",
        "    buffer = io.BytesIO()\n",
        "    c = canvas.Canvas(buffer, pagesize=letter)\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    c.drawString(100, 750, f\"Enviroscan Pollution Report: {time_range}\")\n",
        "    c.drawString(100, 730, f\"Generated: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
        "    c.drawString(100, 700, f\"Stations: {df['location_name'].nunique()}\")\n",
        "    y = 680\n",
        "    for station in df[\"location_name\"].unique():\n",
        "        station_data = df[df[\"location_name\"] == station][POLLUTANTS].mean().round(2)\n",
        "        c.drawString(100, y, f\"Station: {station}\")\n",
        "        for pollutant, value in station_data.items():\n",
        "            c.drawString(120, y - 20, f\"{pollutant}: {value}\")\n",
        "            y -= 20\n",
        "        y -= 20\n",
        "    c.save()\n",
        "    buffer.seek(0)\n",
        "    return buffer\n",
        "\n",
        "def send_email_alert(pollutant, value, station, threshold):\n",
        "    msg = MIMEText(\n",
        "        f\"Alert: {pollutant.upper()} level ({value:.2f}) exceeds threshold ({threshold}) at {station} on {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}.\"\n",
        "    )\n",
        "    msg[\"Subject\"] = f\"Enviroscan Pollution Alert: {pollutant.upper()}\"\n",
        "    msg[\"From\"] = EMAIL_CONFIG[\"sender\"]\n",
        "    msg[\"To\"] = EMAIL_CONFIG[\"receiver\"]\n",
        "\n",
        "    try:\n",
        "        with smtplib.SMTP(EMAIL_CONFIG[\"server\"], EMAIL_CONFIG[\"port\"]) as server:\n",
        "            server.starttls()\n",
        "            server.login(EMAIL_CONFIG[\"sender\"], EMAIL_CONFIG[\"password\"])\n",
        "            server.sendmail(EMAIL_CONFIG[\"sender\"], EMAIL_CONFIG[\"receiver\"], msg.as_string())\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to send email alert: {e}\")\n",
        "        return False\n",
        "\n",
        "def create_folium_map(\n",
        "    df, start_date=None, end_date=None, source_filter=None, location_filter=None, show_heatmap=True, heatmap_field=\"aqi_proxy\"\n",
        "):\n",
        "    required_cols = [\"latitude\", \"longitude\", \"location_name\", \"datetimeUtc\", \"aqi_proxy\", \"pollution_source\"]\n",
        "    if df.empty or not all(col in df.columns for col in required_cols):\n",
        "        st.warning(\"DataFrame is empty or missing required columns for map visualization.\")\n",
        "        return None\n",
        "\n",
        "    df[\"datetimeUtc\"] = pd.to_datetime(df[\"datetimeUtc\"])\n",
        "    if start_date:\n",
        "        df = df[df[\"datetimeUtc\"].dt.date >= start_date]\n",
        "    if end_date:\n",
        "        df = df[df[\"datetimeUtc\"].dt.date <= end_date]\n",
        "    if source_filter and source_filter != \"All\":\n",
        "        df = df[df[\"pollution_source\"] == source_filter]\n",
        "    if location_filter:\n",
        "        df = df[\n",
        "            (df[\"latitude\"] >= location_filter.get(\"min_lat\", -90))\n",
        "            & (df[\"latitude\"] <= location_filter.get(\"max_lat\", 90))\n",
        "            & (df[\"longitude\"] >= location_filter.get(\"min_lon\", -180))\n",
        "            & (df[\"longitude\"] <= location_filter.get(\"max_lon\", 180))\n",
        "        ]\n",
        "\n",
        "    if df.empty:\n",
        "        st.warning(\"No data available after applying filters.\")\n",
        "        return None\n",
        "\n",
        "    aggregated_df = df.groupby(\"location_name\").agg(\n",
        "        {\n",
        "            \"latitude\": \"mean\",\n",
        "            \"longitude\": \"mean\",\n",
        "            \"aqi_proxy\": \"mean\",\n",
        "            \"pollution_source\": \"first\",\n",
        "        }\n",
        "    ).reset_index()\n",
        "\n",
        "    center_lat = aggregated_df[\"latitude\"].mean()\n",
        "    center_lon = aggregated_df[\"longitude\"].mean()\n",
        "    m = folium.Map(location=[center_lat, center_lon], zoom_start=10)\n",
        "\n",
        "    if show_heatmap and heatmap_field in aggregated_df.columns:\n",
        "        aggregated_df[heatmap_field] = pd.to_numeric(aggregated_df[heatmap_field], errors=\"coerce\")\n",
        "        aggregated_df[[\"latitude\", \"longitude\"]] = aggregated_df[[\"latitude\", \"longitude\"]].apply(\n",
        "            pd.to_numeric, errors=\"coerce\"\n",
        "        )\n",
        "        aggregated_df = aggregated_df.dropna(subset=[heatmap_field, \"latitude\", \"longitude\"])\n",
        "        if aggregated_df.empty:\n",
        "            st.warning(\"No valid numeric data for heatmap.\")\n",
        "            return m\n",
        "\n",
        "        valid_values = aggregated_df[heatmap_field].dropna()\n",
        "        if len(valid_values) == 0 or valid_values.max() == valid_values.min():\n",
        "            normalized_val = [1.0] * len(aggregated_df)\n",
        "        else:\n",
        "            normalized_val = (\n",
        "                (aggregated_df[heatmap_field] - valid_values.min()) / (valid_values.max() - valid_values.min())\n",
        "            ).fillna(1.0).tolist()\n",
        "\n",
        "        heat_data = [\n",
        "            [row.latitude, row.longitude, norm_val]\n",
        "            for row, norm_val in zip(aggregated_df.itertuples(), normalized_val)\n",
        "            if pd.notna(norm_val) and pd.notna(row.latitude) and pd.notna(row.longitude)\n",
        "        ]\n",
        "        if heat_data:\n",
        "            HeatMap(heat_data, radius=15, blur=10, gradient={0.0: \"blue\", 0.5: \"lime\", 1.0: \"red\"}).add_to(m)\n",
        "\n",
        "    for _, row in aggregated_df.iterrows():\n",
        "        if pd.isna(row[\"aqi_proxy\"]):\n",
        "            continue\n",
        "        severity_color = (\n",
        "            \"green\"\n",
        "            if row[\"aqi_proxy\"] <= 50\n",
        "            else \"yellow\"\n",
        "            if row[\"aqi_proxy\"] <= 100\n",
        "            else \"orange\"\n",
        "            if row[\"aqi_proxy\"] <= 200\n",
        "            else \"red\"\n",
        "        )\n",
        "        folium.Marker(\n",
        "            location=[row[\"latitude\"], row[\"longitude\"]],\n",
        "            popup=f\"{row['location_name']}<br>AQI Proxy: {row['aqi_proxy']:.2f}<br>Source: {row['pollution_source']}\",\n",
        "            icon=folium.Icon(color=severity_color, icon=\"cloud\", prefix=\"fa\"),\n",
        "        ).add_to(m)\n",
        "\n",
        "    return m\n",
        "\n",
        "# --- Streamlit App ---\n",
        "def main():\n",
        "    st.markdown(\"<h1 style='text-align: center;'>Enviroscan Dashboard 🌱</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "    # --- Session State Initialization ---\n",
        "    if \"df_filtered\" not in st.session_state:\n",
        "        st.session_state.df_filtered = None\n",
        "    if \"processed\" not in st.session_state:\n",
        "        st.session_state.processed = False\n",
        "\n",
        "    # --- Sidebar for Inputs ---\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"<h2 style='color: #2e7d32;'>📊 Control Panel</h2>\", unsafe_allow_html=True)\n",
        "        with st.expander(\"Input Parameters\", expanded=True):\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            with col1:\n",
        "                city = st.text_input(\"City\", value=\"Delhi\", key=\"city_input\", help=\"Enter the city name\")\n",
        "            with col2:\n",
        "                lat = st.number_input(\"Latitude\", value=28.7041, format=\"%.4f\", key=\"lat_input\", help=\"Enter latitude\")\n",
        "            with col3:\n",
        "                lon = st.number_input(\"Longitude\", value=77.1025, format=\"%.4f\", key=\"lon_input\", help=\"Enter longitude\")\n",
        "\n",
        "        with st.expander(\"Time Range\", expanded=True):\n",
        "            col4, col5 = st.columns(2)\n",
        "            with col4:\n",
        "                start_date = st.date_input(\n",
        "                    \"Start Date\", value=datetime(2025, 9, 1), key=\"start_date\", help=\"Select start date\"\n",
        "                )\n",
        "            with col5:\n",
        "                end_date = st.date_input(\n",
        "                    \"End Date\", value=datetime(2025, 9, 15), key=\"end_date\", help=\"Select end date\"\n",
        "                )\n",
        "            time_range = f\"{start_date} to {end_date}\"\n",
        "\n",
        "        uploaded_file = st.file_uploader(\n",
        "            \"Upload CSV File\", type=[\"csv\"], key=\"file_uploader\", help=\"Upload your environmental data CSV\"\n",
        "        )\n",
        "        if st.button(\"Process Data\", key=\"process_button\"):\n",
        "            if uploaded_file:\n",
        "                st.session_state.processed = True\n",
        "                st.session_state.df_filtered = None\n",
        "                st.success(\"Data processing initiated...\")\n",
        "            else:\n",
        "                st.warning(\"Please upload a CSV file.\")\n",
        "\n",
        "    # --- Main Dashboard Layout ---\n",
        "    left_col, right_col = st.columns([2, 1])\n",
        "\n",
        "    with left_col:\n",
        "        st.markdown('<div class=\"card\">', unsafe_allow_html=True)\n",
        "        if st.session_state.processed and uploaded_file is not None:\n",
        "            st.info(\"Processing uploaded file...\")\n",
        "            try:\n",
        "                st.write(\"Debug: Starting build_dataset with city:\", city, \"lat:\", lat, \"lon:\", lon)\n",
        "                df_aq, meta = build_dataset(city, lat, lon, uploaded_file, OPENWEATHER_KEY)\n",
        "                st.write(\"Debug: build_dataset completed, df_aq shape:\", df_aq.shape if not df_aq.empty else \"Empty\")\n",
        "                if not df_aq.empty:\n",
        "                    st.write(f\"**Dataset Summary**: {meta['records']} records, {meta['unique_stations']} unique stations\")\n",
        "                    st.write(\"**Stations**:\", df_aq[\"location_name\"].unique().tolist())\n",
        "                    save_datasets(df_aq, \"delhi_aq_data\")\n",
        "                    save_datasets(meta, \"delhi_meta_data\")\n",
        "                    consolidate_dataset(df_aq, meta, \"delhi_environmental_data\")\n",
        "                    st.success(\"✅ Dataset processing complete.\")\n",
        "\n",
        "                    # --- Data Cleaning ---\n",
        "                    if st.session_state.df_filtered is None:\n",
        "                        with st.spinner(\"Cleaning data...\"):\n",
        "                            df = pd.read_csv(\"delhi_environmental_data.csv\")\n",
        "                            st.write(\"Debug: Loaded df shape:\", df.shape)\n",
        "                            st.write(\"**Columns in DataFrame**:\", df.columns.tolist())\n",
        "                            df[\"datetimeUtc\"] = pd.to_datetime(df[\"datetimeUtc\"], errors=\"coerce\")\n",
        "                            df_filtered = df[\n",
        "                                (df[\"datetimeUtc\"].dt.date >= start_date) & (df[\"datetimeUtc\"].dt.date <= end_date)\n",
        "                            ]\n",
        "                            if df_filtered.empty:\n",
        "                                st.warning(\"No data available for the selected time range. Showing all data.\")\n",
        "                                df_filtered = df.copy()\n",
        "\n",
        "                            pollutant_cols = [col for col in POLLUTANTS if col in df_filtered.columns]\n",
        "                            for col in pollutant_cols:\n",
        "                                df_filtered[col] = df_filtered[col].fillna(df_filtered[col].median())\n",
        "                            weather_cols = [\"temp_c\", \"humidity\", \"pressure\", \"wind_speed\", \"wind_dir\"]\n",
        "                            for col in weather_cols:\n",
        "                                if col in df_filtered.columns:\n",
        "                                    df_filtered[col] = df_filtered[col].fillna(df_filtered[col].mean())\n",
        "                            for col in [\"roads_count\", \"industries_count\", \"farms_count\", \"dumps_count\"]:\n",
        "                                df_filtered[col] = df_filtered.get(col, 0)\n",
        "\n",
        "                            if pollutant_cols:\n",
        "                                df_filtered[\"aqi_proxy\"] = df_filtered[pollutant_cols].mean(axis=1, skipna=True)\n",
        "                            else:\n",
        "                                df_filtered[\"aqi_proxy\"] = 0\n",
        "                                st.warning(\"⚠️ No pollutant columns found, aqi_proxy set to 0 for visualization\")\n",
        "                            if \"pm25\" in df_filtered.columns and \"roads_count\" in df_filtered.columns:\n",
        "                                df_filtered[\"pollution_per_road\"] = df_filtered[\"pm25\"] / (df_filtered[\"roads_count\"] + 1)\n",
        "                            else:\n",
        "                                df_filtered[\"pollution_per_road\"] = np.nan\n",
        "                                st.warning(\"⚠️ pm25 or roads_count missing, skipping pollution_per_road\")\n",
        "\n",
        "                            df_filtered[\"aqi_category\"] = df_filtered[\"aqi_proxy\"].apply(\n",
        "                                lambda x: \"Good\"\n",
        "                                if pd.notna(x) and x <= 50\n",
        "                                else \"Moderate\"\n",
        "                                if pd.notna(x) and x <= 100\n",
        "                                else \"Unhealthy\"\n",
        "                                if pd.notna(x) and x <= 200\n",
        "                                else \"Hazardous\"\n",
        "                            )\n",
        "                            if all(\n",
        "                                col in df_filtered.columns\n",
        "                                for col in [\"pm25\", \"roads_count\", \"industries_count\", \"farms_count\"]\n",
        "                            ):\n",
        "                                df_filtered[\"pollution_source\"] = df_filtered.apply(label_source, axis=1)\n",
        "                            else:\n",
        "                                st.warning(\"⚠️ Required columns for labeling pollution_source are missing.\")\n",
        "                                df_filtered[\"pollution_source\"] = \"Unknown\"\n",
        "\n",
        "                            num_cols = [\n",
        "                                col\n",
        "                                for col in [\n",
        "                                    \"pm25\",\n",
        "                                    \"pm10\",\n",
        "                                    \"no2\",\n",
        "                                    \"co\",\n",
        "                                    \"so2\",\n",
        "                                    \"o3\",\n",
        "                                    \"roads_count\",\n",
        "                                    \"industries_count\",\n",
        "                                    \"farms_count\",\n",
        "                                    \"dumps_count\",\n",
        "                                    \"aqi_proxy\",\n",
        "                                    \"pollution_per_road\",\n",
        "                                ]\n",
        "                                + weather_cols\n",
        "                                if col in df_filtered.columns\n",
        "                            ]\n",
        "                            scaler = StandardScaler()\n",
        "                            df_filtered[num_cols] = scaler.fit_transform(df_filtered[num_cols])\n",
        "                            categorical_cols = [col for col in [\"city\", \"aqi_category\"] if col in df_filtered.columns]\n",
        "                            if categorical_cols:\n",
        "                                df_filtered = pd.get_dummies(df_filtered, columns=categorical_cols, drop_first=True)\n",
        "                            df_filtered.to_csv(\"cleaned_environmental_data.csv\", index=False)\n",
        "                            st.success(\"💾 Cleaned dataset saved as cleaned_environmental_data.csv\")\n",
        "                            st.session_state.df_filtered = df_filtered\n",
        "                else:\n",
        "                    st.warning(\"No data processed from the uploaded file.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing dataset: {str(e)}\")\n",
        "                st.write(\"Debug: Exception occurred, check logs for details.\")\n",
        "            else:\n",
        "                st.warning(\"Please upload a CSV file and click 'Process Data' to start.\")\n",
        "\n",
        "            # Ensure df_filtered is available before proceeding\n",
        "            if \"df_filtered\" in st.session_state and st.session_state.df_filtered is not None:\n",
        "                df_filtered = st.session_state.df_filtered\n",
        "\n",
        "                st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "                # --- Preview Section ---\n",
        "                with st.expander(\"📋 Data Preview\", expanded=True):\n",
        "                    st.write(\"**Unique Stations**:\", df_filtered[\"location_name\"].nunique())\n",
        "                    st.write(\"**Stations**:\", df_filtered[\"location_name\"].unique().tolist())\n",
        "                    if not df_filtered.empty:\n",
        "                        preview_df = df_filtered.groupby(\"location_name\").head(2).reset_index(drop=True)\n",
        "                        st.dataframe(preview_df.style.format({\"aqi_proxy\": \"{:.2f}\"}))\n",
        "                        st.write(\n",
        "                            f\"Displaying up to 2 rows per station. Total stations: {df_filtered['location_name'].nunique()}\"\n",
        "                        )\n",
        "                    else:\n",
        "                        st.warning(\"No data available for preview.\")\n",
        "\n",
        "                # --- Alerts ---\n",
        "                with st.expander(\"🚨 Real-Time Alerts\", expanded=False):\n",
        "                    alert_found = False\n",
        "                    st.write(\"**Monitoring Thresholds**:\", THRESHOLDS)\n",
        "                    for pollutant, threshold in THRESHOLDS.items():\n",
        "                        if pollutant in df_filtered.columns:\n",
        "                            st.write(f\"**Checking {pollutant.upper()}** (Threshold: {threshold})\")\n",
        "                            exceedances = df_filtered[df_filtered[pollutant] > threshold]\n",
        "                            if not exceedances.empty:\n",
        "                                alert_found = True\n",
        "                                st.error(\n",
        "                                    f\"**Alert**: {pollutant.upper()} exceeds threshold ({threshold}) at {len(exceedances)} records!\"\n",
        "                                )\n",
        "                                st.dataframe(\n",
        "                                    exceedances[[\"location_name\", \"datetimeUtc\", pollutant]].style.format({pollutant: \"{:.2f}\"})\n",
        "                                )\n",
        "                                for _, row in exceedances.groupby([\"location_name\", pollutant]).first().reset_index().iterrows():\n",
        "                                    if send_email_alert(pollutant, row[pollutant], row[\"location_name\"], threshold):\n",
        "                                        st.success(f\"Email alert sent for {pollutant.upper()} at {row['location_name']}\")\n",
        "                            else:\n",
        "                                st.write(f\"No {pollutant.upper()} exceedances detected.\")\n",
        "                        else:\n",
        "                            st.warning(f\"⚠️ {pollutant.upper()} data not available in the dataset.\")\n",
        "\n",
        "                    if not alert_found:\n",
        "                        st.warning(\"No alerts triggered. Check if pollutant levels exceed thresholds or if data is available.\")\n",
        "                        st.write(\"**Sample Data Preview for Debugging**:\")\n",
        "                        sample_data = df_filtered[POLLUTANTS + [\"location_name\", \"datetimeUtc\"]].head(5)\n",
        "                        st.dataframe(sample_data.style.format({p: \"{:.2f}\" for p in POLLUTANTS}))\n",
        "\n",
        "                # --- Visual Components ---\n",
        "                with st.expander(\"📊 Visualizations\", expanded=True):\n",
        "                    st.subheader(\"Pollutant Trends\")\n",
        "                    for pollutant in POLLUTANTS:\n",
        "                        if pollutant in df_filtered.columns:\n",
        "                            fig, ax = plt.subplots(figsize=(10, 4))\n",
        "                            for station in df_filtered[\"location_name\"].unique():\n",
        "                                station_data = df_filtered[df_filtered[\"location_name\"] == station]\n",
        "                                ax.plot(station_data[\"datetimeUtc\"], station_data[pollutant], label=station, alpha=0.7)\n",
        "                            ax.set_title(f\"{pollutant.upper()} Trend\")\n",
        "                            ax.set_xlabel(\"Time\")\n",
        "                            ax.set_ylabel(pollutant.upper())\n",
        "                            ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "                            plt.xticks(rotation=45)\n",
        "                            st.pyplot(fig)\n",
        "\n",
        "                    st.subheader(\"Pollution Source Distribution\")\n",
        "                    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "                    source_counts = df_filtered[\"pollution_source\"].value_counts()\n",
        "                    ax.pie(source_counts, labels=source_counts.index, autopct=\"%1.1f%%\", colors=sns.color_palette(\"viridis\", len(source_counts)))\n",
        "                    ax.set_title(\"Predicted Pollution Source Distribution\")\n",
        "                    st.pyplot(fig)\n",
        "\n",
        "                # --- Analysis Options ---\n",
        "                st.subheader(\"⚙️ Analysis Options\")\n",
        "                analysis_option = st.radio(\"Select Analysis:\", (\"Generate Heatmaps Only\", \"Train Models\"), key=\"analysis_option\")\n",
        "\n",
        "                if analysis_option == \"Generate Heatmaps Only\":\n",
        "                    st.info(\"Generating heatmaps...\")\n",
        "                    with st.spinner(\"Rendering map...\"):\n",
        "                        col6, col7, col8 = st.columns(3)\n",
        "                        with col6:\n",
        "                            source_filter = st.selectbox(\n",
        "                                \"Pollution Source\", [\"All\"] + [\"Industrial\", \"Traffic\", \"Agricultural\", \"Mixed/Other\"], key=\"source_filter\"\n",
        "                            )\n",
        "                        with col7:\n",
        "                            show_heatmap = st.checkbox(\"Show Heatmap\", value=True, key=\"show_heatmap\")\n",
        "                            if show_heatmap:\n",
        "                                heatmap_fields = [col for col in [\"aqi_proxy\"] + POLLUTANTS if col in df_filtered.columns]\n",
        "                                heatmap_field = st.selectbox(\"Heatmap Field\", heatmap_fields, key=\"heatmap_field\")\n",
        "                        with col8:\n",
        "                            min_lat = st.number_input(\"Min Latitude\", value=28.0, format=\"%.4f\", key=\"min_lat\")\n",
        "                            max_lat = st.number_input(\"Max Latitude\", value=29.0, format=\"%.4f\", key=\"max_lat\")\n",
        "                            min_lon = st.number_input(\"Min Longitude\", value=76.0, format=\"%.4f\", key=\"min_lon\")\n",
        "                            max_lon = st.number_input(\"Max Longitude\", value=78.0, format=\"%.4f\", key=\"max_lon\")\n",
        "                        location_filter = {\"min_lat\": min_lat, \"max_lat\": max_lat, \"min_lon\": min_lon, \"max_lon\": max_lon}\n",
        "\n",
        "                        map_obj = create_folium_map(df_filtered, start_date, end_date, source_filter, location_filter, show_heatmap, heatmap_field)\n",
        "                        if map_obj:\n",
        "                            st_folium(map_obj, width=700, height=500, key=\"pollution_map\")\n",
        "                        else:\n",
        "                            st.warning(\"Unable to render map due to data issues.\")\n",
        "\n",
        "                elif analysis_option == \"Train Models\":\n",
        "                    if st.button(\"Start Training\", key=\"train_model_button\"):\n",
        "                        st.info(\"Training models...\")\n",
        "                        df_model = df_filtered.dropna(subset=[\"pollution_source\"]).reset_index(drop=True)\n",
        "                        X = df_model.drop(columns=[\"pollution_source\", \"location_name\", \"datetimeUtc\"])\n",
        "                        y = df_model[\"pollution_source\"]\n",
        "                        X = X.select_dtypes(include=[np.number])\n",
        "                        numeric_columns = X.columns.tolist()\n",
        "                        imputer = SimpleImputer(strategy=\"median\")\n",
        "                        X = imputer.fit_transform(X)\n",
        "                        scaler = StandardScaler()\n",
        "                        X = scaler.fit_transform(X)\n",
        "                        models = {\n",
        "                            \"Logistic Regression\": LogisticRegression(max_iter=1000, C=0.1, random_state=42),\n",
        "                            \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "                            \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
        "                        }\n",
        "\n",
        "                        if X.shape[0] < 50:\n",
        "                            for name, model in models.items():\n",
        "                                scores = cross_validate(\n",
        "                                    model,\n",
        "                                    X,\n",
        "                                    y,\n",
        "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                                    scoring=[\"accuracy\", \"precision_weighted\", \"recall_weighted\", \"f1_weighted\"],\n",
        "                                )\n",
        "                                st.write(f\"{name} Cross-Validation Results:\")\n",
        "                                st.write(\n",
        "                                    {k.replace(\"test_\", \"\"): f\"{v.mean():.2f} ± {v.std():.2f}\" for k, v in scores.items()}\n",
        "                                )\n",
        "                        else:\n",
        "                            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "                            if len(y_train.value_counts()) > 1 and min(y_train.value_counts()) > 1:\n",
        "                                smote = SMOTE(random_state=42)\n",
        "                                X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "                                st.write(\"Class distribution after SMOTE:\", pd.Series(y_train).value_counts())\n",
        "                            X_train = imputer.fit_transform(X_train)\n",
        "                            X_test = imputer.transform(X_test)\n",
        "                            X_train = scaler.fit_transform(X_train)\n",
        "                            X_test = scaler.transform(X_test)\n",
        "\n",
        "                            performance = {}\n",
        "                            for name, model in models.items():\n",
        "                                with st.spinner(f\"Training {name}...\"):\n",
        "                                    model.fit(X_train, y_train)\n",
        "                                    y_pred = model.predict(X_test)\n",
        "                                    y_proba = model.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "                                    metrics = {\n",
        "                                        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "                                        \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "                                        \"recall\": recall_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "                                        \"f1\": f1_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "                                    }\n",
        "                                    st.write(f\"✅ {name} Results: {metrics}\")\n",
        "                                    st.text(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "                                    cm = confusion_matrix(y_test, y_pred, labels=y.unique())\n",
        "                                    fig, ax = plt.subplots()\n",
        "                                    sns.heatmap(\n",
        "                                        cm,\n",
        "                                        annot=True,\n",
        "                                        fmt=\"d\",\n",
        "                                        cmap=\"Blues\",\n",
        "                                        xticklabels=y.unique(),\n",
        "                                        yticklabels=y.unique(),\n",
        "                                        ax=ax,\n",
        "                                    )\n",
        "                                    ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", title=f\"Confusion Matrix - {name}\")\n",
        "                                    st.pyplot(fig)\n",
        "\n",
        "                                    model_filename = f\"{name.replace(' ', '_').lower()}_model.pkl\"\n",
        "                                    joblib.dump(model, model_filename)\n",
        "                                    st.success(f\"Model saved as {model_filename}\")\n",
        "                                    performance[name] = metrics\n",
        "\n",
        "                            st.subheader(\"📊 Model Performance Comparison\")\n",
        "                            perf_df = pd.DataFrame(performance).T\n",
        "                            st.dataframe(perf_df.style.format(\"{:.2f}\"))\n",
        "                            best_model = perf_df[\"f1\"].idxmax()\n",
        "                            st.success(f\"🏆 Best model based on F1-score: {best_model}\")\n",
        "\n",
        "                            X_test_orig = pd.DataFrame(X_test, columns=numeric_columns)\n",
        "                            X_test_orig[\"actual_source\"] = y_test.reset_index(drop=True)\n",
        "                            X_test_orig[\"predicted_source\"] = y_pred\n",
        "                            X_test_orig[\"confidence\"] = [max(proba) for proba in y_proba] if y_proba is not None else np.nan\n",
        "                            X_test_orig.to_csv(\"final_predictions.csv\", index=False)\n",
        "                            st.success(\"💾 Final predictions saved as final_predictions.csv\")\n",
        "\n",
        "                # --- Download Options ---\n",
        "                with st.expander(\"📥 Download Reports\", expanded=False):\n",
        "                    latest_date = df_filtered[\"datetimeUtc\"].dt.date.max()\n",
        "                    daily_df = df_filtered[df_filtered[\"datetimeUtc\"].dt.date == latest_date]\n",
        "                    st.download_button(\n",
        "                        \"Download Daily Report (CSV)\",\n",
        "                        data=daily_df.to_csv(index=False),\n",
        "                        file_name=f\"daily_report_{latest_date}.csv\",\n",
        "                        mime=\"text/csv\",\n",
        "                    )\n",
        "                    weekly_df = df_filtered[df_filtered[\"datetimeUtc\"].dt.date >= latest_date - timedelta(days=7)]\n",
        "                    st.download_button(\n",
        "                        \"Download Weekly Report (CSV)\",\n",
        "                        data=weekly_df.to_csv(index=False),\n",
        "                        file_name=f\"weekly_report_{latest_date}.csv\",\n",
        "                        mime=\"text/csv\",\n",
        "                    )\n",
        "                    pdf_buffer = generate_pdf_report(df_filtered, \"pollution_report\", time_range)\n",
        "                    st.download_button(\n",
        "                        \"Download Summary Report (PDF)\",\n",
        "                        data=pdf_buffer,\n",
        "                        file_name=\"pollution_report.pdf\",\n",
        "                        mime=\"application/pdf\",\n",
        "                    )\n",
        "        else:\n",
        "            st.warning(\"Please upload a CSV file and click 'Process Data' to start.\")\n",
        "            st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    with right_col:\n",
        "        st.markdown('<div class=\"card\">', unsafe_allow_html=True)\n",
        "        st.subheader(\"🌍 Pollution Source Insights\")\n",
        "        tab1, tab2, tab3, tab4 = st.tabs([\"Industrial\", \"Traffic\", \"Agricultural\", \"Mixed/Other\"])\n",
        "\n",
        "        with tab1:\n",
        "            st.markdown(\n",
        "                \"\"\"\n",
        "                **Type of Pollutant**: Chemical (e.g., sulfur dioxide, heavy metals) and particulate matter (PM2.5, PM10).\n",
        "                **Medium**: Primarily air, with some water and soil contamination from industrial runoff.\n",
        "                **Health Problems**: Respiratory issues (e.g., bronchitis), cardiovascular diseases, lung cancer, skin irritation.\n",
        "                **Remedies & Precautions**: Install scrubbers and filters in factories, enforce strict emission standards, use protective gear (masks, gloves), promote green belts around industrial zones, regular health check-ups.\n",
        "                **Environmental Impact**: Acid rain, ecosystem degradation, biodiversity loss.\n",
        "                **Global Example**: The Great Smog of London (1952) reduced emissions via the Clean Air Act.\n",
        "                **Preventive Tips**: Avoid proximity during peak production, support sustainable industries.\n",
        "                \"\"\",\n",
        "                unsafe_allow_html=True,\n",
        "            )\n",
        "\n",
        "        with tab2:\n",
        "            st.markdown(\n",
        "                \"\"\"\n",
        "                **Type of Pollutant**: Aerial (e.g., nitrogen oxides, carbon monoxide) and particulate matter.\n",
        "                **Medium**: Air, with minor soil deposition from exhaust.\n",
        "                **Health Problems**: Asthma, allergies, reduced lung function, heart attacks, cognitive decline in children.\n",
        "                **Remedies & Precautions**: Promote electric vehicles, improve public transport, carpooling, use air purifiers indoors, avoid heavy traffic zones.\n",
        "                **Environmental Impact**: Smog formation, ozone depletion, climate change contribution.\n",
        "                **Global Example**: Los Angeles reduced traffic pollution with strict vehicle emission laws.\n",
        "                **Preventive Tips**: Check air quality indexes daily, limit outdoor exercise during high pollution.\n",
        "                \"\"\",\n",
        "                unsafe_allow_html=True,\n",
        "            )\n",
        "\n",
        "        with tab3:\n",
        "            st.markdown(\n",
        "                \"\"\"\n",
        "                **Type of Pollutant**: Chemical (e.g., pesticides, ammonia) and biological (e.g., manure gases).\n",
        "                **Medium**: Air, water (runoff), and soil.\n",
        "                **Health Problems**: Respiratory infections, neurological disorders, waterborne diseases, skin rashes.\n",
        "                **Remedies & Precautions**: Use organic farming, manage livestock waste, install water treatment systems, wear protective clothing during farming.\n",
        "                **Environmental Impact**: Eutrophication of water bodies, soil degradation, loss of pollinators.\n",
        "                **Global Example**: The Netherlands uses precision agriculture to minimize runoff.\n",
        "                **Preventive Tips**: Consume locally sourced organic produce, avoid contaminated water sources.\n",
        "                \"\"\",\n",
        "                unsafe_allow_html=True,\n",
        "            )\n",
        "\n",
        "        with tab4:\n",
        "            st.markdown(\n",
        "                \"\"\"\n",
        "                **Type of Pollutant**: Mixed (chemical, aerial, biological) depending on sources.\n",
        "                **Medium**: Air, water, and soil in varying degrees.\n",
        "                **Health Problems**: Chronic fatigue, immune system weakening, multiple organ issues, cancer risk.\n",
        "                **Remedies & Precautions**: Conduct regular environmental audits, use hybrid mitigation (filters, waste management), public awareness campaigns, personal protective equipment.\n",
        "                **Environmental Impact**: Unpredictable ecosystem shifts, cumulative pollution effects.\n",
        "                **Global Example**: Beijing’s mixed pollution tackled with multi-source regulations.\n",
        "                **Preventive Tips**: Monitor local pollution levels, support community clean-up drives.\n",
        "                \"\"\",\n",
        "                unsafe_allow_html=True,\n",
        "            )\n",
        "\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyngrok\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "ngrok.set_auth_token(\"32W0mx9IJfz2qVFu8Bkl42HLEDR_6TJJr9TpfhFbABpjkjCgC\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"🌍 Streamlit App Live at: {public_url}\")\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py', '--server.port', '8501', '--server.headless', 'true'])\n",
        "time.sleep(5)\n",
        "try:\n",
        "    process.wait()\n",
        "except KeyboardInterrupt:\n",
        "    ngrok.kill()\n",
        "    print(\"Application stopped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOFVR5edkZSb",
        "outputId": "4fbe1a5a-41a0-4c88-8e65-f94bd07037b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌍 Streamlit App Live at: NgrokTunnel: \"https://065fc8d6c702.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "Application stopped.\n"
          ]
        }
      ]
    }
  ]
}